{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 - Medical Imaging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from PIL import Image \n",
    "from matplotlib import pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "from skimage import morphology \n",
    "from skimage.transform import rotate \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "\n",
    "from time import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial parameters for running the notebook \n",
    "PREPROCESS = False\n",
    "COMPUTE_FEATURES = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to directories used \n",
    "\n",
    "ROOT_DIR = \"../\"\n",
    "DATA_DIR = ROOT_DIR + \"data\"\n",
    "# IM_DIR = DATA_DIR + \"/ISIC-2017_Training_Data\"\n",
    "# MASK_DIR = DATA_DIR + \"/ISIC-2017_Training_Part1_GroundTruth\"\n",
    "IM_DIR = DATA_DIR + \"/filtered_images\"\n",
    "MASK_DIR = DATA_DIR + \"/filtered_masks\"\n",
    "DIAGNOSIS_PATH = DATA_DIR + \"/ISIC-2017_Training_Part3_GroundTruth.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all filenames into a dictionary\n",
    "FILENAMES = {}\n",
    "FILENAMES['images'] = sorted([IM_DIR + \"/\" + i for i in list(os.walk(IM_DIR))[0][2]])\n",
    "FILENAMES['masks'] = sorted([f\"{MASK_DIR}/{i}\" for i in list(os.walk(MASK_DIR))[0][2]])\n",
    "FILENAMES['image_num'] = len(FILENAMES['images'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sides_even(image): \n",
    "    '''\n",
    "    Function to make the numbers of columns and rows in \n",
    "    an image even. \n",
    "    Input: An image \n",
    "    Output: An image\n",
    "    '''\n",
    "    # Convert image to numpy array\n",
    "    image = np.array(image)\n",
    "    \n",
    "    # Check if the number of rows is even \n",
    "    if image.shape[0] % 2 != 0: \n",
    "        # Delete the first row\n",
    "        image = np.delete(image,0,axis = 0)\n",
    "    \n",
    "    # Check if the number of columns is even \n",
    "    if image.shape[1] % 2 != 0: \n",
    "        # Delete the first column\n",
    "        image = np.delete(image,0,axis = 1)\n",
    "    \n",
    "    # Convert numpy array back to image \n",
    "    image = Image.fromarray(image)\n",
    "    # Return the updated image \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_crop_image(image_path,mask_path): \n",
    "    '''\n",
    "    Function to filter filter an image based on a given mask \n",
    "    and crop both image and mask to the relevant area. \n",
    "    '''\n",
    "    # Instantiate both image and mask \n",
    "    image = Image.open(image_path) \n",
    "    mask = Image.open(mask_path)\n",
    "    \n",
    "    # Crop both image and mask to the bounding box\n",
    "    # of the mask. \n",
    "    image_crop = image.crop(mask.getbbox())\n",
    "    mask_crop = mask.crop(mask.getbbox())\n",
    "    \n",
    "    # Make the length and height of the the image and \n",
    "    # mask even \n",
    "    image_crop = make_sides_even(image_crop)\n",
    "    mask_crop = make_sides_even(mask_crop)\n",
    "\n",
    "    # Instantiate a blank image for a composite image\n",
    "    tmp_image = Image.new(\"RGB\",image_crop.size, 0)\n",
    "\n",
    "    # Create a composite image based on the blank image, \n",
    "    # the cropped image and the mask \n",
    "    filtered_image = Image.composite(image_crop,tmp_image,mask_crop)\n",
    "    \n",
    "    # Return the filtered image, the cropped mask and the cropped image \n",
    "    return filtered_image, mask_crop, image_crop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the PREPROCESS variable is True \n",
    "if PREPROCESS: \n",
    "    # Try to create directories for the filtered images, cropped masks \n",
    "    # and cropped images \n",
    "    try: \n",
    "        os.makedirs(DATA_DIR + \"/filtered_images\")\n",
    "        os.makedirs(DATA_DIR + \"/filtered_masks\")\n",
    "        os.makedirs(DATA_DIR + \"/cropped_images\")\n",
    "\n",
    "    except FileExistsError:\n",
    "        print(\"Directories exist\")\n",
    "    except IsADirectoryError: \n",
    "        print(\"Directories exist\")\n",
    "\n",
    "    # Instantiate variables containing the paths to the filtered images, \n",
    "    # cropped masks and cropped images \n",
    "    IMAGE_FILTER_DIR = DATA_DIR + \"/filtered_images\"\n",
    "    MASK_FILTER_DIR = DATA_DIR + \"/filtered_masks\"\n",
    "    IMAGE_CROP_DIR = DATA_DIR + \"/cropped_images\"\n",
    "\n",
    "    # Loop over all images and masks based on the number of images\n",
    "    for i in range(FILENAMES['image_num']):\n",
    "        # Instantiate variables with the paths to the currently processing image\n",
    "        # and mask \n",
    "        mask_path = FILENAMES['masks'][i]\n",
    "        image_path = FILENAMES['images'][i]\n",
    "\n",
    "        # Filter and crop the image and mask \n",
    "        image, mask, crop = filter_and_crop_image(image_path,mask_path)\n",
    "\n",
    "        # Instantiate variables containing the image and mask name \n",
    "        image_name = image_path.split(\"/\")[-1].split(\".\")[-2]\n",
    "        mask_name = mask_path.split(\"/\")[-1].split(\".\")[-2]\n",
    "\n",
    "        # Save the generated images and mask to files \n",
    "        image.save(IMAGE_FILTER_DIR + \"/\" + image_name + \".jpg\")\n",
    "        mask.save(MASK_FILTER_DIR + \"/\" + mask_name + \".png\")\n",
    "        crop.save(IMAGE_CROP_DIR + \"/\" + image_name + \".jpg\")\n",
    "\n",
    "        # Clear the temporary variables \n",
    "        del mask_path, image_path, image, mask, crop, image_name, mask_name\n",
    "    \n",
    "    # Load all the new filenames into the FILENAMES dictionary\n",
    "    FILENAMES['filtered_images'] = sorted([f\"{IMAGE_FILTER_DIR}/{i}\" for i in list(os.walk(IMAGE_FILTER_DIR))[0][2]])\n",
    "    FILENAMES['cropped_images'] = sorted([f\"{IMAGE_CROP_DIR}/{i}\" for i in list(os.walk(IMAGE_CROP_DIR))[0][2]])\n",
    "    FILENAMES['cropped_masks'] = sorted([f\"{MASK_FILTER_DIR}/{i}\" for i in list(os.walk(MASK_FILTER_DIR))[0][2]])\n",
    "else: \n",
    "    # Instantiate variables containing the paths to the filtered images, \n",
    "    # cropped masks and cropped images \n",
    "    IMAGE_FILTER_DIR = DATA_DIR + \"/filtered_images\"\n",
    "    MASK_FILTER_DIR = DATA_DIR + \"/filtered_masks\"\n",
    "    IMAGE_CROP_DIR = DATA_DIR + \"/cropped_images\"\n",
    "\n",
    "    # Load all the new filenames into the FILENAMES dictionary\n",
    "    FILENAMES['filtered_images'] = sorted([f\"{IMAGE_FILTER_DIR}/{i}\" for i in list(os.walk(IMAGE_FILTER_DIR))[0][2]])\n",
    "    FILENAMES['cropped_images'] = sorted([f\"{IMAGE_CROP_DIR}/{i}\" for i in list(os.walk(IMAGE_CROP_DIR))[0][2]])\n",
    "    FILENAMES['cropped_masks'] = sorted([f\"{MASK_FILTER_DIR}/{i}\" for i in list(os.walk(MASK_FILTER_DIR))[0][2]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the area and perimeter of the mask \n",
    "def get_area_perimeter(mask, erosion=2): \n",
    "    '''\n",
    "    Function which takes in a mask for a\n",
    "    given image and returns the area and \n",
    "    perimeter of the mask. \n",
    "    '''\n",
    "\n",
    "    # Convert the mask to a Numpy array containing 1's and 0's\n",
    "    mask = np.where(np.array(mask) ==255,1,0)\n",
    "\n",
    "    # Calculate the area of the mask as the sum of the Numpy array\n",
    "    area = np.sum(mask)\n",
    "    \n",
    "    # Erode the edge of the mask in order to calculate the perimeter \n",
    "    mask_erosion = morphology.binary_erosion(mask,morphology.disk(erosion))\n",
    "        \n",
    "    # Calculate the perimeter \n",
    "    perimeter = np.sum(mask - mask_erosion)\n",
    "    # Return the area and the perimeter \n",
    "    return area, perimeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compactness(mask): \n",
    "    '''\n",
    "    Function which takes in a mask for a given \n",
    "    image, calls the get_area_perimeter function\n",
    "    to get the area and perimeter, and returns a \n",
    "    compactness score based upon [CITATION NEEDED]. \n",
    "    '''\n",
    "    # Calculate the area and the perimeter \n",
    "    area, perimeter = get_area_perimeter(mask) \n",
    "    # Calculate the compactness \n",
    "    compactness = perimeter ** 2 / (4 * np.pi * area)\n",
    "    # Return the compactness \n",
    "    return compactness "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_asymmetry(mask, rotation=45): \n",
    "    '''\n",
    "    Takes in a mask for a given image, rotates it \n",
    "    180 times by one degree, compares the left and \n",
    "    right half and returns an average asymmetry score.\n",
    "    '''\n",
    "    # Instantiate the mask as a Numpy array \n",
    "    mask = np.array(mask)\n",
    "    axes = 0 \n",
    "    # Get the length and width of the mask \n",
    "    height, width = mask.shape \n",
    "    \n",
    "    # Get the area of the lesion \n",
    "    lesion_area = len(np.where(mask != 0)[0])\n",
    "\n",
    "    # Instantiate a list to holds the calculated differences  \n",
    "    diffs = []\n",
    "\n",
    "    while axes * rotation < 180:\n",
    "        temp_mask = rotate(mask, axes * rotation)\n",
    "        # length_lesion = np.nonzero(np.sum(temp_mask, axis = 0))[0][-1] - np.nonzero(np.sum(temp_mask, axis = 0))[0][0]\n",
    "\n",
    "        left_mask = temp_mask[:, :int(width/2)] \n",
    "        right_mask = temp_mask[:, int(width/2):]\n",
    "        right_mask = np.fliplr(right_mask)\n",
    "        rotation_diff = np.where(left_mask != right_mask)\n",
    "    \n",
    "        diffs.append(len(rotation_diff[0]))\n",
    "        \n",
    "        axes += 1\n",
    "    \n",
    "    diff = np.mean(diffs)\n",
    "    \n",
    "    return diff / lesion_area \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_luminance(image): \n",
    "    '''\n",
    "    A function which takes in an image and\n",
    "    returns the average luminance. \n",
    "    Input: A PIL Image \n",
    "    Output: Average luminance of the image\n",
    "    '''\n",
    "    # Convert the image to grayscale and then to a Numpy array \n",
    "    grayscale = np.array(image.convert('L'))\n",
    "\n",
    "    # Calculate the mean of the luminance\n",
    "    average_luminance = round(np.mean(grayscale[grayscale > 0]))\n",
    "\n",
    "    # Return the average luminance \n",
    "    return average_luminance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_luminance_variability(image,measure=\"variance\"): \n",
    "\n",
    "    # Convert the image to grayscale and then to a Numpy array \n",
    "    grayscale = np.array(image.convert('L'))\n",
    "\n",
    "    # Check if measure is 'variance'\n",
    "    if measure == 'variance': \n",
    "        # Using numpy's variance method, return the variance in luminance\n",
    "        return round(np.var(grayscale[grayscale > 0]))\n",
    "    # Check if measure is 'standard_deviation'\n",
    "    elif measure == \"standard_deviation\": \n",
    "        # Using numpy's standard deviation method, return the standard deviation\n",
    "        return round(np.std(grayscale[grayscale > 0]))\n",
    "    else: \n",
    "        # If measure is neither 'variance' nor 'standard_deviation', raise a \n",
    "        # ValueError. \n",
    "        raise ValueError(\"Only 'variance' or 'standard_deviation' accepted.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_color(image): \n",
    "    '''\n",
    "    A function which takes in an image and returns \n",
    "    the average color of the image. \n",
    "    '''\n",
    "    # Split the image into separate color channels \n",
    "    r, g, b = image.split()\n",
    "    # Instantiate a numpy array based on each color channel \n",
    "    r = np.array(r)\n",
    "    g = np.array(g)\n",
    "    b = np.array(b)\n",
    "    # Calculate the mean of each color channel \n",
    "    average_color = (\n",
    "        round(np.mean(r[r > 0])), \n",
    "        round(np.mean(g[g > 0])), \n",
    "        round(np.mean(b[b > 0]))\n",
    "    )\n",
    "    # Return a tuple of the average color \n",
    "    return average_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_variance(image,measure=\"variance\"): \n",
    "    '''\n",
    "    A function which takes in an image and \n",
    "    returns the variance of the color. \n",
    "    '''\n",
    "    # Split the image into separate color channels \n",
    "    r, g, b = image.split()\n",
    "    # Instantiate a numpy array based on each color channel \n",
    "    r = np.array(r)\n",
    "    g = np.array(g)\n",
    "    b = np.array(b)\n",
    "    # Check if measure is 'variance'\n",
    "    if measure == \"variance\": \n",
    "        # Using numpy's variance method, calculate the variance of each color\n",
    "        rgb = (\n",
    "            np.var(r[r>0]),\n",
    "            np.var(g[g>0]),\n",
    "            np.var(b[b>0])\n",
    "        )\n",
    "    # Check if measure is 'standard_deviation'\n",
    "    elif measure == \"standard_deviation\": \n",
    "        # Using numpy's standard deviation method, calculate the standard deviation\n",
    "        # of each color \n",
    "        rgb = (\n",
    "            np.std(r[r>0]),\n",
    "            np.std(g[g>0]),\n",
    "            np.std(b[b>0])\n",
    "        )\n",
    "    else: \n",
    "        # If measure is neither 'variance' nor 'standard_deviation', raise \n",
    "        # a ValueError \n",
    "        raise ValueError(\"Only 'variance' or 'standard_deviation' accepted.\") \n",
    "    # Return the mean of the variances or standard deviation of the 3 colors\n",
    "    return np.mean(rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the COMPUTE_FEATURES variable is True \n",
    "if COMPUTE_FEATURES: \n",
    "    #Instantiate a dictionary to store the features and image_id's \n",
    "    feature_dictionary = {\n",
    "        \"image_id\": [],\n",
    "        \"area\": [], \n",
    "        \"perimeter\": [],\n",
    "        \"compactness\": [], \n",
    "        \"asymmetry\": [], \n",
    "        \"luminance_average\": [],\n",
    "        \"luminance_variance\": [],\n",
    "        \"red_average\": [],\n",
    "        \"green_average\": [],\n",
    "        \"blue_average\": [],\n",
    "        \"color_variance\": [],\n",
    "    }\n",
    "\n",
    "    # Loop over all filtered images and cropped masks \n",
    "    for i in range(FILENAMES['image_num']): \n",
    "        # Instantiate variables containing the paths to the image and mask\n",
    "        filtered_image_path = FILENAMES['filtered_images'][i]\n",
    "        cropped_mask_path = FILENAMES['cropped_masks'][i]\n",
    "\n",
    "        # Get the image name from the image path \n",
    "        image_name = filtered_image_path.split(\"/\")[-1].split(\".\")[-2]\n",
    "\n",
    "        print(f\"Currently working on {i} - Image id: {image_name}\")\n",
    "\n",
    "        # Open both the image and mask as PIL Images \n",
    "        filtered_image = Image.open(filtered_image_path)\n",
    "        cropped_mask = Image.open(cropped_mask_path)\n",
    "\n",
    "        # Calculate all features and append them to the relevant list in \n",
    "        # the feature_dictionary \n",
    "        feature_dictionary['image_id'].append(image_name)\n",
    "        area, perimeter = get_area_perimeter(cropped_mask)\n",
    "        feature_dictionary['area'].append(area)\n",
    "        feature_dictionary['perimeter'].append(perimeter)\n",
    "        feature_dictionary['compactness'].append(get_compactness(cropped_mask))\n",
    "        feature_dictionary['asymmetry'].append(get_asymmetry(cropped_mask))\n",
    "        feature_dictionary['luminance_average'].append(get_average_luminance(filtered_image))\n",
    "        feature_dictionary['luminance_variance'].append(get_luminance_variability(filtered_image))\n",
    "        red, green, blue = get_avg_color(filtered_image)\n",
    "        feature_dictionary['red_average'].append(red)\n",
    "        feature_dictionary['green_average'].append(green)\n",
    "        feature_dictionary['blue_average'].append(blue)\n",
    "        feature_dictionary['color_variance'].append(get_color_variance(filtered_image))\n",
    "    # Instantiate a pandas DataFrame based on the feature_dictionary\n",
    "    features = pd.DataFrame(feature_dictionary)\n",
    "    # Write the features DataFrame to .csv \n",
    "    features.to_csv(ROOT_DIR + \"/features/feature_set.csv\",sep=\";\",index=False)\n",
    "else: \n",
    "    # Read the previously calculated feature set into a pandas DataFrame \n",
    "    features = pd.read_csv(ROOT_DIR + \"/features/feature_set.csv\", sep=\";\", index_col=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create list of features \n",
    "feature_list = list(features.columns)[1:]\n",
    "\n",
    "\n",
    "## Load the Melanoma data \n",
    "data = pd.read_csv(\"../data/ISIC-2017_Training_Part3_GroundTruth.csv\")\n",
    "\n",
    "## Merge the Melanoma data with the feature data \n",
    "image_data = pd.merge(features, data, on = \"image_id\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>area</th>\n",
       "      <th>perimeter</th>\n",
       "      <th>compactness</th>\n",
       "      <th>asymmetry</th>\n",
       "      <th>luminance_average</th>\n",
       "      <th>luminance_variance</th>\n",
       "      <th>red_average</th>\n",
       "      <th>green_average</th>\n",
       "      <th>blue_average</th>\n",
       "      <th>color_variance</th>\n",
       "      <th>melanoma</th>\n",
       "      <th>seborrheic_keratosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_0000000</td>\n",
       "      <td>364956</td>\n",
       "      <td>3906</td>\n",
       "      <td>3.326704</td>\n",
       "      <td>0.169375</td>\n",
       "      <td>79</td>\n",
       "      <td>1617</td>\n",
       "      <td>85</td>\n",
       "      <td>77</td>\n",
       "      <td>82</td>\n",
       "      <td>1646.168808</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0000001</td>\n",
       "      <td>53426</td>\n",
       "      <td>1773</td>\n",
       "      <td>4.682254</td>\n",
       "      <td>0.075609</td>\n",
       "      <td>54</td>\n",
       "      <td>892</td>\n",
       "      <td>73</td>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>837.143114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_0000002</td>\n",
       "      <td>237402</td>\n",
       "      <td>4097</td>\n",
       "      <td>5.626492</td>\n",
       "      <td>0.127264</td>\n",
       "      <td>109</td>\n",
       "      <td>1210</td>\n",
       "      <td>126</td>\n",
       "      <td>104</td>\n",
       "      <td>107</td>\n",
       "      <td>1202.416805</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_0000003</td>\n",
       "      <td>260708</td>\n",
       "      <td>3639</td>\n",
       "      <td>4.042033</td>\n",
       "      <td>0.090658</td>\n",
       "      <td>100</td>\n",
       "      <td>1506</td>\n",
       "      <td>130</td>\n",
       "      <td>93</td>\n",
       "      <td>68</td>\n",
       "      <td>1392.834772</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_0000004</td>\n",
       "      <td>269476</td>\n",
       "      <td>3392</td>\n",
       "      <td>3.397674</td>\n",
       "      <td>0.118065</td>\n",
       "      <td>143</td>\n",
       "      <td>1562</td>\n",
       "      <td>183</td>\n",
       "      <td>125</td>\n",
       "      <td>150</td>\n",
       "      <td>1711.177114</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>ISIC_0015220</td>\n",
       "      <td>21598298</td>\n",
       "      <td>36340</td>\n",
       "      <td>4.865645</td>\n",
       "      <td>0.109164</td>\n",
       "      <td>82</td>\n",
       "      <td>885</td>\n",
       "      <td>140</td>\n",
       "      <td>59</td>\n",
       "      <td>49</td>\n",
       "      <td>909.184420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>ISIC_0015233</td>\n",
       "      <td>7283040</td>\n",
       "      <td>18275</td>\n",
       "      <td>3.649154</td>\n",
       "      <td>0.126046</td>\n",
       "      <td>90</td>\n",
       "      <td>1887</td>\n",
       "      <td>119</td>\n",
       "      <td>80</td>\n",
       "      <td>63</td>\n",
       "      <td>1887.810925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>ISIC_0015260</td>\n",
       "      <td>5988478</td>\n",
       "      <td>21771</td>\n",
       "      <td>6.298403</td>\n",
       "      <td>0.155695</td>\n",
       "      <td>57</td>\n",
       "      <td>1230</td>\n",
       "      <td>80</td>\n",
       "      <td>50</td>\n",
       "      <td>34</td>\n",
       "      <td>1191.485062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>ISIC_0015284</td>\n",
       "      <td>11498389</td>\n",
       "      <td>24030</td>\n",
       "      <td>3.996324</td>\n",
       "      <td>0.102036</td>\n",
       "      <td>97</td>\n",
       "      <td>1989</td>\n",
       "      <td>137</td>\n",
       "      <td>84</td>\n",
       "      <td>66</td>\n",
       "      <td>2045.950121</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>ISIC_0015295</td>\n",
       "      <td>21653812</td>\n",
       "      <td>38322</td>\n",
       "      <td>5.396996</td>\n",
       "      <td>0.140494</td>\n",
       "      <td>113</td>\n",
       "      <td>1053</td>\n",
       "      <td>136</td>\n",
       "      <td>108</td>\n",
       "      <td>81</td>\n",
       "      <td>1057.249956</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          image_id      area  perimeter  compactness  asymmetry  \\\n",
       "0     ISIC_0000000    364956       3906     3.326704   0.169375   \n",
       "1     ISIC_0000001     53426       1773     4.682254   0.075609   \n",
       "2     ISIC_0000002    237402       4097     5.626492   0.127264   \n",
       "3     ISIC_0000003    260708       3639     4.042033   0.090658   \n",
       "4     ISIC_0000004    269476       3392     3.397674   0.118065   \n",
       "...            ...       ...        ...          ...        ...   \n",
       "1995  ISIC_0015220  21598298      36340     4.865645   0.109164   \n",
       "1996  ISIC_0015233   7283040      18275     3.649154   0.126046   \n",
       "1997  ISIC_0015260   5988478      21771     6.298403   0.155695   \n",
       "1998  ISIC_0015284  11498389      24030     3.996324   0.102036   \n",
       "1999  ISIC_0015295  21653812      38322     5.396996   0.140494   \n",
       "\n",
       "      luminance_average  luminance_variance  red_average  green_average  \\\n",
       "0                    79                1617           85             77   \n",
       "1                    54                 892           73             50   \n",
       "2                   109                1210          126            104   \n",
       "3                   100                1506          130             93   \n",
       "4                   143                1562          183            125   \n",
       "...                 ...                 ...          ...            ...   \n",
       "1995                 82                 885          140             59   \n",
       "1996                 90                1887          119             80   \n",
       "1997                 57                1230           80             50   \n",
       "1998                 97                1989          137             84   \n",
       "1999                113                1053          136            108   \n",
       "\n",
       "      blue_average  color_variance  melanoma  seborrheic_keratosis  \n",
       "0               82     1646.168808       0.0                   0.0  \n",
       "1               40      837.143114       0.0                   0.0  \n",
       "2              107     1202.416805       1.0                   0.0  \n",
       "3               68     1392.834772       0.0                   0.0  \n",
       "4              150     1711.177114       1.0                   0.0  \n",
       "...            ...             ...       ...                   ...  \n",
       "1995            49      909.184420       0.0                   1.0  \n",
       "1996            63     1887.810925       0.0                   1.0  \n",
       "1997            34     1191.485062       0.0                   1.0  \n",
       "1998            66     2045.950121       1.0                   0.0  \n",
       "1999            81     1057.249956       0.0                   1.0  \n",
       "\n",
       "[2000 rows x 13 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature Dataframe and melanoma DataFrame\n",
    "X = image_data[feature_list]\n",
    "y = image_data['melanoma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split into development data and test data \n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalize using Scikit-Learn's StandardScaler \n",
    "scaler = StandardScaler().fit(X_dev)\n",
    "X_dev = pd.DataFrame(scaler.transform(X_dev.values), columns=X_dev.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test.values), columns=X_test.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\n",
      "\n",
      "perimeter\n",
      "\n",
      "k = 2\n",
      "\n",
      "perimeter\n",
      "asymmetry\n",
      "\n",
      "k = 3\n",
      "\n",
      "perimeter\n",
      "asymmetry\n",
      "area\n",
      "\n",
      "k = 4\n",
      "\n",
      "perimeter\n",
      "asymmetry\n",
      "luminance_average\n",
      "area\n",
      "\n",
      "k = 5\n",
      "\n",
      "perimeter\n",
      "red_average\n",
      "asymmetry\n",
      "area\n",
      "Noise\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Create Noise columns \n",
    "noise_cols = 4 \n",
    "features_noise = feature_list + ['Noise' for _ in range(noise_cols)]\n",
    "noise = np.random.RandomState(1).uniform(0,0.1, size = (noise_cols, X_dev.shape[0])).transpose()\n",
    "\n",
    "\n",
    "# Merge noise columns with the feature columns \n",
    "X_select = np.hstack((X_dev,noise))\n",
    "y_select = y_dev \n",
    "\n",
    "\n",
    "for k in range(1, 5+1): \n",
    "    kbest = SelectKBest(mutual_info_classif, k=k)\n",
    "    kbest.fit(X_select, y_select)\n",
    "    scores = kbest.scores_ \n",
    "\n",
    "    print(f\"k = {k}\\n\")\n",
    "    score_dict = {scores[i]: i for i in range(len(scores))}\n",
    "    #print(sorted(score_dict))\n",
    "    for val in sorted(score_dict, reverse=True)[:k]:\n",
    "        # print(score_dict[val])\n",
    "        print(features_noise[score_dict[val]])\n",
    "    print()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the addition of Age and Sex to the test set \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the Melanoma data \n",
    "data = pd.read_csv(\"../data/ISIC-2017_Training_Part3_GroundTruth.csv\")\n",
    "\n",
    "## Load the Age and Sex data \n",
    "\n",
    "age_sex = pd.read_csv(\"../data/ISIC-2017_Training_Data_metadata.csv\")\n",
    "\n",
    "\n",
    "## Merge the Age and Sex data with the merged \n",
    "merged_df = pd.merge(image_data, age_sex, on = 'image_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image_id',\n",
       " 'area',\n",
       " 'perimeter',\n",
       " 'compactness',\n",
       " 'asymmetry',\n",
       " 'luminance_average',\n",
       " 'luminance_variance',\n",
       " 'red_average',\n",
       " 'green_average',\n",
       " 'blue_average',\n",
       " 'color_variance',\n",
       " 'melanoma',\n",
       " 'seborrheic_keratosis',\n",
       " 'age_approximate',\n",
       " 'sex']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(merged_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \"unknown\" to none  \n",
    "merged_df['age_approximate'] = [i if i != \"unknown\" else None for i in merged_df['age_approximate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'male' to -1, 'female' to 1 and 'unknown' to none \n",
    "merged_df['sex'] = [-1 if i == \"male\" else 1 if i == \"female\" else None for i in merged_df['sex']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature list \n",
    "column_list = list(merged_df.columns)\n",
    "feature_list = []\n",
    "for i in column_list:\n",
    "    if i != 'melanoma' and i != 'image_id' and i !=  'seborrheic_keratosis': \n",
    "        feature_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with null values \n",
    "merged_without_none = merged_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1744\n"
     ]
    }
   ],
   "source": [
    "print(len(merged_without_none))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature Dataframe and melanoma DataFrame\n",
    "X = merged_without_none[feature_list]\n",
    "y = merged_without_none['melanoma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split into development data and test data \n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalize using Scikit-Learn's StandardScaler \n",
    "scaler = StandardScaler().fit(X_dev)\n",
    "X_dev = pd.DataFrame(scaler.transform(X_dev.values), columns=X_dev.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test.values), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\n",
      "\n",
      "age_approximate\n",
      "k = 2\n",
      "\n",
      "age_approximate\n",
      "perimeter\n",
      "k = 3\n",
      "\n",
      "age_approximate\n",
      "perimeter\n",
      "area\n",
      "k = 4\n",
      "\n",
      "age_approximate\n",
      "perimeter\n",
      "sex\n",
      "area\n",
      "k = 5\n",
      "\n",
      "age_approximate\n",
      "sex\n",
      "perimeter\n",
      "area\n",
      "Noise\n"
     ]
    }
   ],
   "source": [
    "## Create Noise columns \n",
    "noise_cols = 4 \n",
    "features_noise = feature_list + ['Noise' for _ in range(noise_cols)]\n",
    "noise = np.random.RandomState(1).uniform(0,0.1, size = (noise_cols, X_dev.shape[0])).transpose()\n",
    "\n",
    "\n",
    "# Merge noise columns with the feature columns \n",
    "X_select = np.hstack((X_dev,noise))\n",
    "y_select = y_dev \n",
    "\n",
    "\n",
    "for k in range(1, 5+1): \n",
    "    kbest = SelectKBest(mutual_info_classif, k=k)\n",
    "    kbest.fit(X_select, y_select)\n",
    "    scores = kbest.scores_ \n",
    "\n",
    "    print(f\"k = {k}\\n\")\n",
    "    score_dict = {scores[i]: i for i in range(len(scores))}\n",
    "    # print(sorted(score_dict))\n",
    "    for val in sorted(score_dict, reverse=True)[:k]:\n",
    "        # print(score_dict[val])\n",
    "        print(features_noise[score_dict[val]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Tables and Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-34-6cb08df0833b>:24: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  X_val2, y_val2 = X_noisy_scaled_df[X_noisy_scaled_df[\"separator\"] > 15][X_noisy_scaled_df[\"separator\"] < 31], y[X_noisy_scaled_df[\"separator\"] > 15][X_noisy_scaled_df[\"separator\"] < 31]\n",
      "<ipython-input-34-6cb08df0833b>:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  X_val3, y_val3 = X_noisy_scaled_df[X_noisy_scaled_df[\"separator\"] > 30][X_noisy_scaled_df[\"separator\"] < 46], y[X_noisy_scaled_df[\"separator\"] > 30][X_noisy_scaled_df[\"separator\"] < 46]\n",
      "<ipython-input-34-6cb08df0833b>:26: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  X_val4, y_val4 = X_noisy_scaled_df[X_noisy_scaled_df[\"separator\"] > 45][X_noisy_scaled_df[\"separator\"] < 61], y[X_noisy_scaled_df[\"separator\"] > 45][X_noisy_scaled_df[\"separator\"] < 61]\n",
      "<ipython-input-34-6cb08df0833b>:27: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  X_val5, y_val5 = X_noisy_scaled_df[X_noisy_scaled_df[\"separator\"] > 60][X_noisy_scaled_df[\"separator\"] < 76], y[X_noisy_scaled_df[\"separator\"] > 60][X_noisy_scaled_df[\"separator\"] < 76]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# According to the features selection best features are perimeter, asymmetry and red_average\n",
    "\n",
    "feature_list = [\"perimeter\", \"asymmetry\", \"red_average\"]\n",
    "\n",
    "\n",
    "# Creates a random variable between 1 and 100 to facilitate splitting the data into multiple sets\n",
    "np.random.seed(0)\n",
    "separator = np.random.randint(1, 101, size = (image_data.shape[0], 1))\n",
    "\n",
    "# Creates 20 random noise variable to avoid overfitting\n",
    "noise = pd.DataFrame(data = np.random.RandomState(23).uniform(0, 0.1, size = (image_data.shape[0], 20)), columns = [i for i in range(20)])\n",
    "\n",
    "X_main = image_data[feature_list]\n",
    "X_noisy = pd.merge(X_main, noise, left_index = True, right_index = True)\n",
    "\n",
    "X_noisy_scaled = StandardScaler().fit_transform(X_noisy.values)\n",
    "X_noisy_scaled_df = pd.DataFrame(X_noisy_scaled, index=X_noisy.index, columns=X_noisy.columns)\n",
    "X_noisy_scaled_df[\"separator\"] = separator\n",
    "\n",
    "y = image_data[\"melanoma\"]\n",
    "\n",
    "#Separating the dataset into 5 different validation sets and 1 test set\n",
    "X_val1, y_val1 = X_noisy_scaled_df[X_noisy_scaled_df[\"separator\"] < 16], y[X_noisy_scaled_df[\"separator\"] < 16]\n",
    "X_val2, y_val2 = X_noisy_scaled_df[X_noisy_scaled_df[\"separator\"] > 15][X_noisy_scaled_df[\"separator\"] < 31], y[X_noisy_scaled_df[\"separator\"] > 15][X_noisy_scaled_df[\"separator\"] < 31]\n",
    "X_val3, y_val3 = X_noisy_scaled_df[X_noisy_scaled_df[\"separator\"] > 30][X_noisy_scaled_df[\"separator\"] < 46], y[X_noisy_scaled_df[\"separator\"] > 30][X_noisy_scaled_df[\"separator\"] < 46]\n",
    "X_val4, y_val4 = X_noisy_scaled_df[X_noisy_scaled_df[\"separator\"] > 45][X_noisy_scaled_df[\"separator\"] < 61], y[X_noisy_scaled_df[\"separator\"] > 45][X_noisy_scaled_df[\"separator\"] < 61]\n",
    "X_val5, y_val5 = X_noisy_scaled_df[X_noisy_scaled_df[\"separator\"] > 60][X_noisy_scaled_df[\"separator\"] < 76], y[X_noisy_scaled_df[\"separator\"] > 60][X_noisy_scaled_df[\"separator\"] < 76]\n",
    "X_test, y_test= X_noisy_scaled_df[X_noisy_scaled_df[\"separator\"] > 75], y[X_noisy_scaled_df[\"separator\"] > 75]\n",
    "\n",
    "validation_sets = [(X_val1, y_val1), (X_val2, y_val2), (X_val3, y_val3), (X_val4, y_val4), (X_val5, y_val5)]\n",
    "\n",
    "#Creating the training sets from the validation sets\n",
    "X_train1 = pd.concat([X_val2, X_val3, X_val4, X_val5])\n",
    "X_train2 = pd.concat([X_val1, X_val3, X_val4, X_val5])\n",
    "X_train3 = pd.concat([X_val1, X_val2, X_val4, X_val5])\n",
    "X_train4 = pd.concat([X_val1, X_val2, X_val3, X_val5])\n",
    "X_train5 = pd.concat([X_val1, X_val2, X_val3, X_val4])\n",
    "\n",
    "y_train1 = pd.concat([y_val2, y_val3, y_val4, y_val5])\n",
    "y_train2 = pd.concat([y_val1, y_val3, y_val4, y_val5])\n",
    "y_train3 = pd.concat([y_val1, y_val2, y_val4, y_val5])\n",
    "y_train4 = pd.concat([y_val1, y_val2, y_val3, y_val5])\n",
    "y_train5 = pd.concat([y_val1, y_val2, y_val3, y_val4])\n",
    "\n",
    "training_sets = [(X_train1, y_train1), (X_train2, y_train2), (X_train3, y_train3), (X_train4, y_train4), (X_train5, y_train5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the classifiers\n",
    "\n",
    "classifiers_name = [\"KNN_1\", \"KNN_3\", \"KNN_5\", \"KNN_10\", \"KNN_50\", \"Tree\", \"Gaussian\"]\n",
    "classifiers = []\n",
    "classifiers.append(KNeighborsClassifier(n_neighbors = 1))\n",
    "classifiers.append(KNeighborsClassifier(n_neighbors = 3))\n",
    "classifiers.append(KNeighborsClassifier(n_neighbors = 5))\n",
    "classifiers.append(KNeighborsClassifier(n_neighbors = 10))\n",
    "classifiers.append(KNeighborsClassifier(n_neighbors = 50))\n",
    "classifiers.append(DecisionTreeClassifier())\n",
    "classifiers.append(GaussianProcessClassifier())\n",
    "\n",
    "accuracy_scores = []\n",
    "auc_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "\n",
    "# Calculating the scores for each classifier for each training set\n",
    "for i in range(len(training_sets)):\n",
    "    trained_classifiers = [classifier.fit(training_sets[i][0], training_sets[i][1]) for classifier in classifiers]\n",
    "    predictions = [trained.predict(validation_sets[i][0]) for trained in trained_classifiers]\n",
    "    accuracy_scores.append([accuracy_score(validation_sets[i][1], prediction) for prediction in predictions])\n",
    "    auc_scores.append([roc_auc_score(validation_sets[i][1], prediction) for prediction in predictions])\n",
    "    f1_scores.append([f1_score(validation_sets[i][1], prediction) for prediction in predictions])\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "accuracy_scores_avg = []\n",
    "auc_scores_avg = []\n",
    "f1_scores_avg = []\n",
    "\n",
    "# Calculating the average score for each classifier\n",
    "for i in range(len(classifiers)):\n",
    "    accuracy_scores_avg.append(np.mean([accuracy_scores[j][i] for j in range(len(training_sets))]))\n",
    "    auc_scores_avg.append(np.mean([auc_scores[j][i] for j in range(len(training_sets))]))\n",
    "    f1_scores_avg.append(np.mean([f1_scores[j][i] for j in range(len(training_sets))]))\n",
    "\n",
    "\n",
    "columns = [\"Classifier\", \"Accuracy score\", \"Roc Auc score\", \"F1 score\"]\n",
    "data = np.array([classifiers_name, np.round(accuracy_scores_avg, decimals = 5), \n",
    "                 np.round(auc_scores_avg, decimals = 5), np.round(f1_scores_avg, decimals = 5)]).T\n",
    "Classifier_evaluation = pd.DataFrame(data = data, columns = columns)\n",
    "Classifier_evaluation.to_csv(ROOT_DIR + \"/features/Classifier_Evaluation\",sep=\";\",index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7222222222222222\n",
      "0.5408998988877655\n",
      "0.24870466321243523\n"
     ]
    }
   ],
   "source": [
    "# Testing the best classifier (decision tree) on the test data\n",
    "\n",
    "y_test_predict = trained_classifiers[5].predict(X_test)\n",
    "\n",
    "acc_test = accuracy_score(y_test, y_test_predict)\n",
    "auc_test = roc_auc_score(y_test, y_test_predict)\n",
    "f1_test = f1_score(y_test, y_test_predict)\n",
    "\n",
    "print(acc_test)\n",
    "print(auc_test)\n",
    "print(f1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age and Sex Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-40-f4984570cc68>:27: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  X_val2_ageSex, y_val2_ageSex = X_noisy_scaled_df_ageSex[X_noisy_scaled_df_ageSex[\"separator\"] > 15][X_noisy_scaled_df_ageSex[\"separator\"] < 31], y_ageSex[X_noisy_scaled_df_ageSex[\"separator\"] > 15][X_noisy_scaled_df_ageSex[\"separator\"] < 31]\n",
      "<ipython-input-40-f4984570cc68>:28: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  X_val3_ageSex, y_val3_ageSex = X_noisy_scaled_df_ageSex[X_noisy_scaled_df_ageSex[\"separator\"] > 30][X_noisy_scaled_df_ageSex[\"separator\"] < 46], y_ageSex[X_noisy_scaled_df_ageSex[\"separator\"] > 30][X_noisy_scaled_df_ageSex[\"separator\"] < 46]\n",
      "<ipython-input-40-f4984570cc68>:29: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  X_val4_ageSex, y_val4_ageSex = X_noisy_scaled_df_ageSex[X_noisy_scaled_df_ageSex[\"separator\"] > 45][X_noisy_scaled_df_ageSex[\"separator\"] < 61], y_ageSex[X_noisy_scaled_df_ageSex[\"separator\"] > 45][X_noisy_scaled_df_ageSex[\"separator\"] < 61]\n",
      "<ipython-input-40-f4984570cc68>:30: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  X_val5_ageSex, y_val5_ageSex = X_noisy_scaled_df_ageSex[X_noisy_scaled_df_ageSex[\"separator\"] > 60][X_noisy_scaled_df_ageSex[\"separator\"] < 76], y_ageSex[X_noisy_scaled_df_ageSex[\"separator\"] > 60][X_noisy_scaled_df_ageSex[\"separator\"] < 76]\n"
     ]
    }
   ],
   "source": [
    "# Age and Sex classifiers, features are age_approximate, sex, perimeter, area\n",
    "\n",
    "\n",
    "feature_list = [\"age_approximate\", \"sex\", \"perimeter\", \"area\"]\n",
    "\n",
    "\n",
    "# Creates a random variable between 1 and 100 to facilitate splitting the data into multiple sets\n",
    "np.random.seed()\n",
    "separator_ageSex = np.random.randint(1, 101, size = (merged_without_none.shape[0], 1))\n",
    "\n",
    "# Creates 20 random noise variable to avoid overfitting\n",
    "noise_ageSex = pd.DataFrame(data = np.random.RandomState(23).uniform(0, 0.1, size = (merged_without_none.shape[0], 20)), columns = [i for i in range(20)])\n",
    "\n",
    "merged_without_none = merged_without_none.reset_index(drop=True)\n",
    "\n",
    "X_ageSex = merged_without_none[feature_list]\n",
    "X_noisy_ageSex = pd.merge(X_ageSex, noise_ageSex, left_index = True, right_index = True)\n",
    "\n",
    "\n",
    "X_noisy_scaled_ageSex = StandardScaler().fit_transform(X_noisy_ageSex.values)\n",
    "X_noisy_scaled_df_ageSex = pd.DataFrame(X_noisy_scaled_ageSex, index=X_noisy_ageSex.index, columns=X_noisy_ageSex.columns)\n",
    "X_noisy_scaled_df_ageSex[\"separator\"] = separator_ageSex\n",
    "y_ageSex = merged_without_none[\"melanoma\"]\n",
    "\n",
    "#Separating the dataset into 5 different validation sets and 1 test set\n",
    "X_val1_ageSex, y_val1_ageSex = X_noisy_scaled_df_ageSex[X_noisy_scaled_df_ageSex[\"separator\"] < 16], y_ageSex[X_noisy_scaled_df_ageSex[\"separator\"] < 16]\n",
    "X_val2_ageSex, y_val2_ageSex = X_noisy_scaled_df_ageSex[X_noisy_scaled_df_ageSex[\"separator\"] > 15][X_noisy_scaled_df_ageSex[\"separator\"] < 31], y_ageSex[X_noisy_scaled_df_ageSex[\"separator\"] > 15][X_noisy_scaled_df_ageSex[\"separator\"] < 31]\n",
    "X_val3_ageSex, y_val3_ageSex = X_noisy_scaled_df_ageSex[X_noisy_scaled_df_ageSex[\"separator\"] > 30][X_noisy_scaled_df_ageSex[\"separator\"] < 46], y_ageSex[X_noisy_scaled_df_ageSex[\"separator\"] > 30][X_noisy_scaled_df_ageSex[\"separator\"] < 46]\n",
    "X_val4_ageSex, y_val4_ageSex = X_noisy_scaled_df_ageSex[X_noisy_scaled_df_ageSex[\"separator\"] > 45][X_noisy_scaled_df_ageSex[\"separator\"] < 61], y_ageSex[X_noisy_scaled_df_ageSex[\"separator\"] > 45][X_noisy_scaled_df_ageSex[\"separator\"] < 61]\n",
    "X_val5_ageSex, y_val5_ageSex = X_noisy_scaled_df_ageSex[X_noisy_scaled_df_ageSex[\"separator\"] > 60][X_noisy_scaled_df_ageSex[\"separator\"] < 76], y_ageSex[X_noisy_scaled_df_ageSex[\"separator\"] > 60][X_noisy_scaled_df_ageSex[\"separator\"] < 76]\n",
    "X_test_ageSex, y_test_ageSex= X_noisy_scaled_df_ageSex[X_noisy_scaled_df_ageSex[\"separator\"] > 75], y_ageSex[X_noisy_scaled_df_ageSex[\"separator\"] > 75]\n",
    "\n",
    "validation_sets_ageSex = [(X_val1_ageSex, y_val1_ageSex), (X_val2_ageSex, y_val2_ageSex), (X_val3_ageSex, y_val3_ageSex), (X_val4_ageSex, y_val4_ageSex), (X_val5_ageSex, y_val5_ageSex)]\n",
    "\n",
    "#Creating the training sets from the validation sets\n",
    "X_train1_ageSex = pd.concat([X_val2_ageSex, X_val3_ageSex, X_val4_ageSex, X_val5_ageSex])\n",
    "X_train2_ageSex = pd.concat([X_val1_ageSex, X_val3_ageSex, X_val4_ageSex, X_val5_ageSex])\n",
    "X_train3_ageSex = pd.concat([X_val1_ageSex, X_val2_ageSex, X_val4_ageSex, X_val5_ageSex])\n",
    "X_train4_ageSex = pd.concat([X_val1_ageSex, X_val2_ageSex, X_val3_ageSex, X_val5_ageSex])\n",
    "X_train5_ageSex = pd.concat([X_val1_ageSex, X_val2_ageSex, X_val3_ageSex, X_val4_ageSex])\n",
    "\n",
    "y_train1_ageSex = pd.concat([y_val2_ageSex, y_val3_ageSex, y_val4_ageSex, y_val5_ageSex])\n",
    "y_train2_ageSex = pd.concat([y_val1_ageSex, y_val3_ageSex, y_val4_ageSex, y_val5_ageSex])\n",
    "y_train3_ageSex = pd.concat([y_val1_ageSex, y_val2_ageSex, y_val4_ageSex, y_val5_ageSex])\n",
    "y_train4_ageSex = pd.concat([y_val1_ageSex, y_val2_ageSex, y_val3_ageSex, y_val5_ageSex])\n",
    "y_train5_ageSex = pd.concat([y_val1_ageSex, y_val2_ageSex, y_val3_ageSex, y_val4_ageSex])\n",
    "\n",
    "training_sets_ageSex = [(X_train1_ageSex, y_train1_ageSex), (X_train2_ageSex, y_train2_ageSex), (X_train3_ageSex, y_train3_ageSex), (X_train4_ageSex, y_train4_ageSex), (X_train5_ageSex, y_train5_ageSex)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the classifiers\n",
    "\n",
    "classifiers_name = [\"KNN_1\", \"KNN_3\", \"KNN_5\", \"KNN_10\", \"KNN_50\", \"Tree\", \"Gaussian\"]\n",
    "classifiers_ageSex = []\n",
    "classifiers_ageSex.append(KNeighborsClassifier(n_neighbors = 1))\n",
    "classifiers_ageSex.append(KNeighborsClassifier(n_neighbors = 3))\n",
    "classifiers_ageSex.append(KNeighborsClassifier(n_neighbors = 5))\n",
    "classifiers_ageSex.append(KNeighborsClassifier(n_neighbors = 10))\n",
    "classifiers_ageSex.append(KNeighborsClassifier(n_neighbors = 50))\n",
    "classifiers_ageSex.append(DecisionTreeClassifier())\n",
    "classifiers_ageSex.append(GaussianProcessClassifier())\n",
    "\n",
    "\n",
    "accuracy_scores_ageSex = []\n",
    "auc_scores_ageSex = []\n",
    "f1_scores_ageSex = []\n",
    "\n",
    "\n",
    "# Calculating the scores for each classifier for each training set\n",
    "for i in range(len(training_sets_ageSex)):\n",
    "    trained_classifiers_ageSex = [classifier.fit(training_sets_ageSex[i][0], training_sets_ageSex[i][1]) for classifier in classifiers_ageSex]\n",
    "    predictions_ageSex = [trained.predict(validation_sets_ageSex[i][0]) for trained in trained_classifiers_ageSex]\n",
    "    accuracy_scores_ageSex.append([accuracy_score(validation_sets_ageSex[i][1], prediction) for prediction in predictions_ageSex])\n",
    "    auc_scores_ageSex.append([roc_auc_score(validation_sets_ageSex[i][1], prediction) for prediction in predictions_ageSex])\n",
    "    f1_scores_ageSex.append([f1_score(validation_sets_ageSex[i][1], prediction) for prediction in predictions_ageSex])\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "accuracy_scores_ageSex_avg = []\n",
    "auc_scores_ageSex_avg = []\n",
    "f1_scores_ageSex_avg = []\n",
    "\n",
    "# Calculating the average score for each classifier\n",
    "for i in range(len(classifiers_ageSex)):\n",
    "    accuracy_scores_ageSex_avg.append(np.mean([accuracy_scores_ageSex[j][i] for j in range(len(training_sets_ageSex))]))\n",
    "    auc_scores_ageSex_avg.append(np.mean([auc_scores_ageSex[j][i] for j in range(len(training_sets_ageSex))]))\n",
    "    f1_scores_ageSex_avg.append(np.mean([f1_scores_ageSex[j][i] for j in range(len(training_sets_ageSex))]))\n",
    "\n",
    "\n",
    "columns = [\"Classifier\", \"Accuracy score\", \"Roc Auc score\", \"F1 score\"]\n",
    "data = np.array([classifiers_name, np.round(accuracy_scores_ageSex_avg, decimals = 5), \n",
    "                 np.round(auc_scores_ageSex_avg, decimals = 5), np.round(f1_scores_ageSex_avg, decimals = 5)]).T\n",
    "Classifier_evaluation_ageSex = pd.DataFrame(data = data, columns = columns)\n",
    "Classifier_evaluation_ageSex.to_csv(ROOT_DIR + \"/features/Classifier_Evaluation_ageSex\",sep=\";\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6956521739130435\n",
      "0.5290114613180515\n",
      "0.2485875706214689\n"
     ]
    }
   ],
   "source": [
    "# Testing the best classifier (decision tree) on the test data\n",
    "\n",
    "y_test_predict_ageSex = trained_classifiers_ageSex[5].predict(X_test_ageSex)\n",
    "\n",
    "acc_test_ageSex = accuracy_score(y_test_ageSex, y_test_predict_ageSex)\n",
    "auc_test_ageSex = roc_auc_score(y_test_ageSex, y_test_predict_ageSex)\n",
    "f1_test_ageSex = f1_score(y_test_ageSex, y_test_predict_ageSex)\n",
    "\n",
    "print(acc_test_ageSex)\n",
    "print(auc_test_ageSex)\n",
    "print(f1_test_ageSex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
